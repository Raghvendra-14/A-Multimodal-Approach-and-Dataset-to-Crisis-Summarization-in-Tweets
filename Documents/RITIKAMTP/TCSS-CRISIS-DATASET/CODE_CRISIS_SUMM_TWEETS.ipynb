{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8RWtrhDm0to"
      },
      "source": [
        "**PREPROCSSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0n4JqCzj3vN"
      },
      "outputs": [],
      "source": [
        "!pip install nltk scikit-learn numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7xmt9synma0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Download required NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def load_tweets(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data['tweets']  # Assuming tweets are stored under 'tweets' key\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    # Remove URLs, mentions, hashtags, and non-alphabetic characters\n",
        "    tweet = re.sub(r'http\\S+|@\\S+|#\\S+', '', tweet)\n",
        "\n",
        "    # Tokenize and lowercase\n",
        "    words = word_tokenize(tweet.lower())\n",
        "\n",
        "    # Initialize lemmatizer and stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Lemmatize and remove stopwords and non-alphabetic tokens\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def preprocess_tweets(tweets):\n",
        "    preprocessed_tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
        "    return tweets, preprocessed_tweets\n",
        "\n",
        "def main():\n",
        "    # Load the tweet file\n",
        "    file_path = 'tweetsample.json'\n",
        "    tweets = load_tweets(file_path)\n",
        "\n",
        "    # Preprocess the tweets\n",
        "    tweets, preprocessed_tweets = preprocess_tweets(tweets)\n",
        "\n",
        "    print(\"Original Tweets:\\n\", tweets)\n",
        "    print(\"\\nPreprocessed Tweets:\\n\", preprocessed_tweets)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRACTIVE PHASE**"
      ],
      "metadata": {
        "id": "6gGg1RjUGilO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Load SpaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def get_bigrams(preprocessed_tweets):\n",
        "    bigrams = []\n",
        "\n",
        "    for tweet in preprocessed_tweets:\n",
        "        doc = nlp(tweet)\n",
        "        tokens = [token.text for token in doc if token.is_alpha]\n",
        "\n",
        "        # Generate bigrams (pairs of adjacent tokens)\n",
        "        bigrams += [f'{tokens[i]} {tokens[i+1]}' for i in range(len(tokens)-1)]\n",
        "\n",
        "    return bigrams\n",
        "\n",
        "def calculate_tfidf_bigrams(preprocessed_tweets, top_k=10):\n",
        "    # Generate bigrams\n",
        "    bigrams = get_bigrams(preprocessed_tweets)\n",
        "\n",
        "    # Calculate TF-IDF for bigrams\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(bigrams)\n",
        "\n",
        "    # Get the top k bigrams based on TF-IDF score\n",
        "    bigram_scores = np.sum(tfidf_matrix.toarray(), axis=0)\n",
        "    top_bigram_indices = np.argsort(bigram_scores)[-top_k:]\n",
        "    top_bigrams = [vectorizer.get_feature_names_out()[i] for i in top_bigram_indices]\n",
        "\n",
        "    return top_bigrams\n",
        "\n",
        "def retrieve_tweets_with_bigrams(tweets, preprocessed_tweets, top_bigrams):\n",
        "    selected_tweets = []\n",
        "\n",
        "    for original_tweet, preprocessed_tweet in zip(tweets, preprocessed_tweets):\n",
        "        # Check if any of the top bigrams appear in the tweet\n",
        "        for bigram in top_bigrams:\n",
        "            if bigram in preprocessed_tweet:\n",
        "                selected_tweets.append(original_tweet)\n",
        "                break  # Avoid adding the same tweet multiple times\n",
        "\n",
        "    return selected_tweets\n",
        "\n",
        "def rank_tweets(tweets, preprocessed_tweets, top_k=10):\n",
        "    # Step 1: Get top bigrams based on TF-IDF\n",
        "    top_bigrams = calculate_tfidf_bigrams(preprocessed_tweets, top_k)\n",
        "\n",
        "    # Step 2: Retrieve tweets containing these bigrams\n",
        "    selected_tweets = retrieve_tweets_with_bigrams(tweets, preprocessed_tweets, top_bigrams)\n",
        "\n",
        "    # Step 3: Concatenate the selected tweets to form the summary\n",
        "    summary = ' '.join(selected_tweets)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def main():\n",
        "    # Load the tweet file\n",
        "    file_path = 'tweetsample.json'\n",
        "    tweets = load_tweets(file_path)\n",
        "\n",
        "    # Preprocess the tweets\n",
        "    tweets, preprocessed_tweets = preprocess_tweets(tweets)\n",
        "\n",
        "    # Generate extractive summary based on bigram ranking\n",
        "    summary = rank_tweets(tweets, preprocessed_tweets, top_k=10)\n",
        "\n",
        "    print(\"\\nExtractive Summary of Tweets:\\n\", summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Z4BmNSzzGsCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ABSTRACTIVE PHASE**"
      ],
      "metadata": {
        "id": "XIKv2UgjHBtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGJfbQrRr6oO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file to get the abstractive summary\n",
        "with open('summary_output.json', 'r') as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "abstractive_summary = data['abstractive_summary']\n",
        "print(abstractive_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XOoGF4ur9QN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Path to the images folder\n",
        "images_folder = 'images'\n",
        "\n",
        "# Load images from the folder\n",
        "image_files = [os.path.join(images_folder, img) for img in os.listdir(images_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "images = [Image.open(img_file) for img_file in image_files]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp4r7MIjsTUA"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# Load CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Prepare CLIP inputs\n",
        "texts = [abstractive_summary] * len(images)  # Create a list with the summary repeated for each image\n",
        "\n",
        "# Process images and texts\n",
        "image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
        "\n",
        "# Move tensors to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
        "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "# Calculate image-text similarity scores\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    outputs = model(**{**image_inputs, **text_inputs})\n",
        "\n",
        "logits_per_image = outputs.logits_per_image  # Image-text similarity scores\n",
        "probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n",
        "\n",
        "# Check the shape of probs to ensure it matches your expectations\n",
        "print(probs.shape)\n",
        "\n",
        "# Extract top 10 image indices\n",
        "num_top_images = min(10, probs.shape[1])  # Ensure we only take as many as are available\n",
        "top_indices = torch.topk(probs, num_top_images, dim=1).indices[0].cpu().numpy()\n",
        "\n",
        "# Print the names of the top 10 images\n",
        "print(f\"Top indices: {top_indices}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV5-VuJpyWhj"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BLIP-2 model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate caption\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "    return generated_text\n",
        "\n",
        "# Load top 10 image paths\n",
        "top_images = [image_files[i] for i in top_indices]\n",
        "print(top_images)\n",
        "\n",
        "# Generate captions for top 10 images\n",
        "captions = [generate_caption(img_path) for img_path in top_images]\n",
        "\n",
        "# Concatenate captions\n",
        "concatenated_captions = \" \".join(captions)\n",
        "\n",
        "print(concatenated_captions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukeYBiuVzVUA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n",
        "\n",
        "# Load BigBird-Pegasus model and tokenizer\n",
        "bigbird_model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "bigbird_tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "\n",
        "# Tokenize concatenated captions\n",
        "inputs = bigbird_tokenizer([concatenated_captions], max_length=4096, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = bigbird_model.generate(inputs[\"input_ids\"], num_beams=4, max_length=200)\n",
        "summary = bigbird_tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "# Print or save the summary\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJBosSDOyXuK"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Prepare CLIP inputs\n",
        "texts = [summary] * len(top_images)  # Create a list with the summary repeated for each image\n",
        "text_inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
        "image_inputs = clip_processor(images=[Image.open(img_path).convert(\"RGB\") for img_path in top_images], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Move tensors to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model.to(device)\n",
        "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
        "image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
        "\n",
        "# Calculate image-text similarity scores\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    outputs = clip_model(**{**image_inputs, **text_inputs})\n",
        "\n",
        "logits_per_image = outputs.logits_per_image  # Image-text similarity scores\n",
        "probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n",
        "\n",
        "# Extract top 4 image indices\n",
        "num_top_images = min(4, probs.shape[1])  # Ensure we only take as many as are available\n",
        "top_indices = torch.topk(probs, num_top_images, dim=1).indices[0].cpu().numpy()\n",
        "\n",
        "# Print the names of the top 4 images\n",
        "print(f\"Top indices: {top_indices}\")\n",
        "\n",
        "# Load top 2 image paths\n",
        "top_4_images = [top_images[i] for i in top_indices]\n",
        "\n",
        "# Print or save the top 4 images\n",
        "for idx, img_path in enumerate(top_4_images):\n",
        "    print(f\"Top image {idx + 1}: {img_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}