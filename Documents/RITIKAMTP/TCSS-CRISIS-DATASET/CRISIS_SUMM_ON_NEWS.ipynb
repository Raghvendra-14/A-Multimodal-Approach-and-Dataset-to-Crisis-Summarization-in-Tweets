{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8RWtrhDm0to"
      },
      "source": [
        "**PREPROCSSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0n4JqCzj3vN"
      },
      "outputs": [],
      "source": [
        "!pip install nltk scikit-learn numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7xmt9synma0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data['body']  # Assuming the text is under the key 'body'\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer and stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
        "        preprocessed_sentences.append(' '.join(words))\n",
        "\n",
        "    return sentences, preprocessed_sentences\n",
        "\n",
        "def rank_sentences(sentences, preprocessed_sentences, top_n=3):\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "\n",
        "    # Compute sentence scores\n",
        "    sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "    # Get top N sentences\n",
        "    top_sentence_indices = np.argsort(sentence_scores)[-top_n:]\n",
        "\n",
        "    # Sort the indices to maintain the original order of sentences\n",
        "    top_sentence_indices.sort()\n",
        "\n",
        "    # Extract the top sentences\n",
        "    top_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "def main():\n",
        "    # Load the JSON file\n",
        "    file_path = 'bbcsample.json'\n",
        "    text = load_json(file_path)\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences, preprocessed_sentences = preprocess_text(text)\n",
        "\n",
        "    # Generate extractive summary\n",
        "    summary = rank_sentences(sentences, preprocessed_sentences, top_n=10)\n",
        "\n",
        "    print(\"Original Text:\\n\", text)\n",
        "    print(\"\\nExtractive Summary:\\n\", summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnJ46eqWo0-C"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n",
        "\n",
        "# Download required NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data['body']  # Assuming the text is under the key 'body'\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer and stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
        "        preprocessed_sentences.append(' '.join(words))\n",
        "\n",
        "    return sentences, preprocessed_sentences\n",
        "\n",
        "def rank_sentences(sentences, preprocessed_sentences, top_n=30):\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "\n",
        "    # Compute sentence scores\n",
        "    sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "    # Get top N sentences\n",
        "    top_sentence_indices = np.argsort(sentence_scores)[-top_n:]\n",
        "\n",
        "    # Sort the indices to maintain the original order of sentences\n",
        "    top_sentence_indices.sort()\n",
        "\n",
        "    # Extract the top sentences\n",
        "    top_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "def generate_abstractive_summary(extractive_summary):\n",
        "    model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "\n",
        "    inputs = tokenizer([extractive_summary], max_length=4096, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Generate Summary\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=500)\n",
        "    abstractive_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    return abstractive_summary\n",
        "\n",
        "def main():\n",
        "    # Load the JSON file\n",
        "    file_path = 'bbcsample.json'\n",
        "    text = load_json(file_path)\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences, preprocessed_sentences = preprocess_text(text)\n",
        "\n",
        "    # Generate extractive summary\n",
        "    extractive_summary = rank_sentences(sentences, preprocessed_sentences, top_n=30)\n",
        "\n",
        "    # Generate abstractive summary\n",
        "    abstractive_summary = generate_abstractive_summary(extractive_summary)\n",
        "\n",
        "    print(\"Original Text:\\n\", text)\n",
        "    print(\"\\nExtractive Summary:\\n\", extractive_summary)\n",
        "    print(\"\\nAbstractive Summary:\\n\", abstractive_summary)\n",
        "\n",
        "    # Save abstractive summary to JSON file\n",
        "    output = {\n",
        "        \"extractive_summary\": extractive_summary,\n",
        "        \"abstractive_summary\": abstractive_summary\n",
        "    }\n",
        "\n",
        "    with open('summary_output.json', 'w') as outfile:\n",
        "      json.dump(output, outfile)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGJfbQrRr6oO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file to get the abstractive summary\n",
        "with open('summary_output.json', 'r') as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "abstractive_summary = data['abstractive_summary']\n",
        "print(abstractive_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XOoGF4ur9QN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Path to the images folder\n",
        "images_folder = 'images'\n",
        "\n",
        "# Load images from the folder\n",
        "image_files = [os.path.join(images_folder, img) for img in os.listdir(images_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "images = [Image.open(img_file) for img_file in image_files]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp4r7MIjsTUA"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# Load CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Prepare CLIP inputs\n",
        "texts = [abstractive_summary] * len(images)  # Create a list with the summary repeated for each image\n",
        "\n",
        "# Process images and texts\n",
        "image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
        "\n",
        "# Move tensors to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
        "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "# Calculate image-text similarity scores\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    outputs = model(**{**image_inputs, **text_inputs})\n",
        "\n",
        "logits_per_image = outputs.logits_per_image  # Image-text similarity scores\n",
        "probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n",
        "\n",
        "# Check the shape of probs to ensure it matches your expectations\n",
        "print(probs.shape)\n",
        "\n",
        "# Extract top 5 image indices\n",
        "num_top_images = min(5, probs.shape[1])  # Ensure we only take as many as are available\n",
        "top_indices = torch.topk(probs, num_top_images, dim=1).indices[0].cpu().numpy()\n",
        "\n",
        "# Print the names of the top 5 images\n",
        "print(f\"Top indices: {top_indices}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV5-VuJpyWhj"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BLIP-2 model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate caption\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "    return generated_text\n",
        "\n",
        "# Load top 5 image paths\n",
        "top_images = [image_files[i] for i in top_indices]\n",
        "print(top_images)\n",
        "\n",
        "# Generate captions for top 5 images\n",
        "captions = [generate_caption(img_path) for img_path in top_images]\n",
        "\n",
        "# Concatenate captions\n",
        "concatenated_captions = \" \".join(captions)\n",
        "\n",
        "print(concatenated_captions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukeYBiuVzVUA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n",
        "\n",
        "# Load BigBird-Pegasus model and tokenizer\n",
        "bigbird_model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "bigbird_tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "\n",
        "# Tokenize concatenated captions\n",
        "inputs = bigbird_tokenizer([concatenated_captions], max_length=4096, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = bigbird_model.generate(inputs[\"input_ids\"], num_beams=4, max_length=200)\n",
        "summary = bigbird_tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "# Print or save the summary\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJBosSDOyXuK"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Prepare CLIP inputs\n",
        "texts = [summary] * len(top_images)  # Create a list with the summary repeated for each image\n",
        "text_inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
        "image_inputs = clip_processor(images=[Image.open(img_path).convert(\"RGB\") for img_path in top_images], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Move tensors to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model.to(device)\n",
        "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
        "image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
        "\n",
        "# Calculate image-text similarity scores\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    outputs = clip_model(**{**image_inputs, **text_inputs})\n",
        "\n",
        "logits_per_image = outputs.logits_per_image  # Image-text similarity scores\n",
        "probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n",
        "\n",
        "# Extract top 2 image indices\n",
        "num_top_images = min(2, probs.shape[1])  # Ensure we only take as many as are available\n",
        "top_indices = torch.topk(probs, num_top_images, dim=1).indices[0].cpu().numpy()\n",
        "\n",
        "# Print the names of the top 2 images\n",
        "print(f\"Top indices: {top_indices}\")\n",
        "\n",
        "# Load top 2 image paths\n",
        "top_2_images = [top_images[i] for i in top_indices]\n",
        "\n",
        "# Print or save the top 2 images\n",
        "for idx, img_path in enumerate(top_2_images):\n",
        "    print(f\"Top image {idx + 1}: {img_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}